---
layout: post
title: 排序算法小结
category: blog
description: 包含了一些常用的排序算法的基本思想和时间复杂度，同时也有一些自己的想法

tags:
- sort
- algorithm
---

这篇Post主要是感觉通过找工同学的反馈，感觉排序算法在笔试和面试中还是比较重要的，另外，之前对这些算法只是有了笼统的认识，还没有比较系统的去接触，所以正好借着这些机会去再系统的接触一下，好建立起一个良好的认识。

## 比较排序

所谓比较排序，就是，在排序的结果中，各元素的次序是基于输入元素间的比较而得到的。例如插入排序、归并排序（合并排序）、堆排序、快速排序等。

### 插入排序（冒泡、选择）

插入排序最直接的实例就是扑克牌，手中的牌都是已经排好序的，每次抽到的牌都要从一端开始比较，例如，从小端，则当出现比当前牌大的，则当前牌就应该放在该位置。

算法流程：
	
	INSERTION_SORT(A)
		for j :  2 to length(A)
			do 	key = A[j]
				insert A[j] into the sorted sequence A[1..j-1]
				i = j - 1
				while i>0 and A[i]>key
				do	A[i+1] = A[i]
					i = i - 1
				A[i+1] = key

插入排序在插入的操作中可以利用一些数据结构来减少位置寻找的时候，比如利用二分法，进而使得时间复杂度降低到`O(n log n)`。

与插入排序类似的排序算法还有选择排序和冒泡排序。这两种方法的思想是一致的，即都是通过每次的遍历选出当前序列中的极值，并放置在相应的位置上，不同的是两者产生极值的方式不同，选择排序是通过和极值位上的数值进行比较并交换，而冒泡排序则是通过相邻两个位置的数值比较和交换一步一步的更新到极值位置。
	
插入排序和选择排序，冒泡排序都是比较直观的排序算法，一般都是生活中常见的排序方法的抽象版，而且算法的复杂度都是`O(n^2)`，插入排序在利用二分法查找插入位置后，时间复杂度可以降低到`O(n log n)`。

另有[希尔排序](http://zh.wikipedia.org/wiki/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F)与插入排序类似，区别主要在于每次比较前进的步长不同，根据步长的不同，算法的时间复杂度不等，最优复杂度为`O(n (log n)^2)`。
	
### 归并（合并）排序
	
归并排序的排序思想是建立在递归分治之上的，即将现有的序列分为两段，分别对各段进行排序，然后将两段有序队列合并为一个有序队列。这个操作过程是递归的，直至到最后序列只有两个元素，按需对两个元素进行交换。
	
算法流程：

	MERGE-SORT(A, p, r)
		if p < r
			then 
				q = floor((p+r)/2)
				MERGE-SORT(A, p, q)
				MERGE-SORT(A, q+1, r)
				MERGE(A, p, q, r)
	
其中MERGE-SORT为递归调用，而MERGE为合并操作。关于合并操作，其实比较常用的是将两个子有序列的头元素进行比较，选择较大（或者较小）的元素取出，然后更新头元素指针，做同样的操作，这样可以保证当前取到的元素一定是这两个序列中最大（或者最小）的。
					
由于归并排序的递归特性，其时间复杂度可以用递推式的形式表达，其中设n为序列的元素个数，T(n)为长度为n的序列进行归并排序的时间复杂度。

>	当n=1时，T(n) = O(1)

>	当n>1时，T(n) = 2T(n/2)+O(n)

其中，当n>1时，前一项为对半分的两个子归并排序的时间代价，后一项的O(n)为合并过程的时间代价。最终可以得到归并排序的时间复杂度为`O(n log n)`。

这里有个小例子，是算法导论上的一道思考题，如何用归并排序的算法结构求出一个序列中的逆序对？这个小例子可以加深对比较排序的具体理解。关于这个的解答，我认为应该是，在合并的过程中，每当从后一个序列中取出头元素时，就将前一个序列所剩的元素数累加起来，因为这时，不论前一个序列中还剩什么样的元素，他们都与当前取出的这个元素成逆序对。

### 堆排序

这里要先引入一个`原地排序`的概念：在任何时候，序列中只有常数个元素存储在输入数组以外，则称为原地排序。其实，顾名思义，就是不需要额外的存储空间，只通过原序列中的交换就可以完成排序。

堆排序中使用了堆这样一个数据结构，关于堆，其实是一棵[完全二叉树](http://zh.wikipedia.org/zh-cn/%E4%BA%8C%E5%8F%89%E6%A0%91)，它有一个特性，对于`大根堆`来说，**其每个分叉处都满足父节点的值大于子节点**，`小根堆`类似。

这里简单记一下完全二叉树的存储方法，由于完全二叉树的特性，使得其可以存在一位数组里，并且下标特点如下：如果当前节点为i，则其父节点的下标为floor(i/2)，其左子节点下标为2i，右子节点下标为2i+1。

对于堆来说，有两个基本的操作，一个是建堆，一个是调整堆。建堆相当与堆的初始化，而调整堆，则是在堆出现元素增加和减少时为了保持堆特性所需要做的一些操作。

以大根堆为例，建堆和调整的流程：

	BUILD-MAX-HEAP(A)
		heap-size[A] = length[A]
		for i : floor(length[A]/2) downto 1 do
			MAX-HEAPIFY(A, i)
		
	MAX-HEAPIFY(A, i)
		l = LEFT(i)
		r = RIGHT(i)
		if l <= heap-size[A] and A[l] > A[i]
			then largest = l
			else largest = i
		if r <= heap-size[A] and A[r] > A[largest]
			then largest = r
		if largest != i
			then 
				exchange A[i]<>A[largest]
				MAX-HEAPIFY(A, largest)

从伪代码可以看出，其实建堆的过程就是一个不断调整堆的过程，这里有一点要注意一下，就是BUILD-MAX-HEAP中的循环是从floor(length[A]/2)开始的，这是完全二叉树的特性之一，floor(length[A]/2)是这个二叉树最下面的一个内节点，floor(length[A]/2)+1就是第一个叶节点。而调整堆的话只需要从最底层的父节点开始就可以，因为叶节点没有子节点，没有调整的必要。另一个原因是，要想要保证上层根节点可以正确更新，则需要下层的根节点首先更新。

在调整堆的MAX-HEAPIFY过程中，要做的其实只有一点就是保持堆中**根大于子节点**的这一特性，所以，在三个值中选择一个较大的并将较大的值的位置存起来最后与根所在位置的值进行交换已更新根，如果某个子节点大于根节点，则交换完后需要更新以该子节点为根的子树，即调用MAX-HEAPIFY(A, largest)，这样可以保证每次上层的更新能及时传递到下层。

可以看到，在大根堆的例子中，最终的根上的值是这棵树中最大的值（小根堆类似），所以，堆排序利用的正是这个特点，每次取出堆顶元素，然后重新调整堆产生新的堆顶元素，如此往复直到所有元素都取完，堆排序的流程为：

	HEAPSORT(A)
		BUILD-MAX-HEAP(A)
		for i : length[A] downto 2 do
			exchange A[1]<>A[i]
			heap-size[A] = heap-size[A] - 1
			MAX-HEAPIFY(A, 1)
			
用heap-size限定了堆的范围，从而分割了未排序和已排序的部分。	

HEAPSORT的时间代价是`O(n log n)`，其中，BUILD-MAX-HEAP的时间复杂度为`O(n)`，MAX-HEAPIFY的时间复杂度为`O(log n)`。
		
#### 优先队列

`优先队列`是一中用来维护由一组元素构成的集合S的数据结构，这种数据结构的特点是有一个可以进行排序的key值，而且只关心key值中的极值。典型的应用场景为操作系统中的调度，每次都需要将优先级最高元素的拿出来调度，而不关心其他元素。

### 快速排序

快速排序是现在应用很广的排序算法，各种库中的排序算法基本都是实现的快速排序。快排也是一种原地排序，不需要太多的额外空间，同时是一种比较排序，最后它的思想也是基于递归分治。

快速排序的关键在每次对序列的划分是否平衡，即是否能选择合适的值将序列尽可能的平分成两个自序列，当然如果合适值的选取代价太大也不可取，而现实中这个“合适的值”往往是随意取的，例如当前序列的头元素，亦或是一个随机位置上的元素。

一个样例的划分流程如下：

	PARTITION(A, p, r)
		x = A[r]
		i = p - 1
		for j : p to r - 1 do
			if A[j] <= x
			then
				i = i + 1
				exchange A[i]<>A[j]
		exchange A[i+1]<>A[r]
	return i+1

这个过程最后返回的是中间的分割点，这个点把序列分成了两个自序列。然后这个分割点的确定是在选取了序列最后一个元素作为主元（pivot element）后，将小于等于它的放在它前面，大于它的放在它后面，从而分成了两个子序列。伪代码中的i是用来标记最左边的一个大于主元的元素的位置，当在这个元素的右边找到有小于主元的元素，就交换这两个元素值。

快排的完成实现流程是：

	QUICKSORT(A, p, r)
		if p < r
		then
			q = PARTITION(A, p, r)
			QUICKSORT(A, p, q-1)
			QUICKSORT(A, q+1, r)

结构很清晰，在PARTITION确定了分割点后递归对自序列进行排序，这里和归并排序的区别在与，这里在对子序列进行排序前就已经将元素的区域划分在PARTITION中做好了，不用在后续进行合并了。

#### 时间复杂度

- 最坏情况：每次划分的自序列都是分别包含n-1和1和元素，T(n) = T(n-1) + T(0) + O(n) = T(n-1) + O(n) => T(n) = O(n^2)
- 最佳情况：每次划分都能平分当前序列，T(n) <= 2T(n/2) + O(n) => T(n) = O(n log n)

首先，明确一点，快排的平均复杂度是趋向于最佳复杂度的，可能是因为在PARTITION中**任何**一种按常数比例的划分都会产生深度为O(log n)的递归树，其中每一层的代价为O(n)，所以总的运行时间都是O(n log n)，具体的原因应该是要涉及到了概率方面的推导。
			
### 比较排序的算法时间下界

比较排序可以抽象成一颗决策树，而这棵决策树的每个节点都是序列中两个元素的比较，从一个高的角度来看，这些比较的顺序可以是任意的，因为无论任何两个元素比较其产生的效果都是把接下来可能出现的情况一分为二，而比较排序中也主要依靠比较来不断的缩小正确序列的范围，抽象出来也就类似与决策树，一种信息上的不断二分，最终确定。

![decision tree](http://kang-da.tk/images/post/comparision-sort-decision-tree.png)

对于一个有n个元素的序列，设其对应的决策树高为h，可达的叶节点数为l。其可能产生的排列有n!个，决策数至少有n!个叶节点，即n！<= l，同时决策树是一棵完全二叉树，其叶节点不多于2^h，即l<=2^h。即可得到n! <= 2^h ==> h >= log (n!) = O(n log n) ([Stirling's approximation](http://en.wikipedia.org/wiki/Stirling%27s_approximation))。决策树高决定了排序算法的比较次数，而对于比较排序而言，比较次数决定了其时间复杂度。所以，可以得出结论，**比较排序的最坏运行时间的下界为`O(n log n)`**。

另外，再引入一个概念，最佳上界能够与上段所得到的非平凡下界渐近地相等，则这个排序算法是`渐近最优`的，归并排序和堆排序都是渐近最优的，而快速排序不是，因为它的最坏时间复杂度是O(n^2)。

## 线性时间排序

与比较排序相对比，这类排序算法不依赖与元素的比较，所以笼统的称为非比较排序，其实它们的另一个特点是都是`线性时间`的排序，即时间复杂度是`O(n)`级别的，但是这些排序算法往往尤其局限性，需要数据满足某些特性。

### 计数排序

计数排序，从这个名字可以看出，它是通过统计来完成排序的，而计数排序的基本思想也基本如此。但是计数排序有其局限性，由于它的实现方式，所以只适用于数据范围比较小的数据。其算法流程是：

	COUNTING-SORT(A, B, k)
		for i : 0 to k do
			C[i] = 0
		for j : 1 to length[A] do
			C[A[j]] = C[A[j]] + 1
		for i : 1 to k do
			C[i] = C[i-1] + C[i]
		for j : length[A] downto 1 do
			B[C[A[j]]] = A[j]
			C[A[j]] = C[A[j]] - 1

可以看出，计数排序的基本思想是，建立一个数组，对每一个可能出现的数据值建立一个统计，则最后对所有数据值从小到大（或者从大到小）按照统计值的数量进行输出即可。当然，这样的方法完全依赖于数据值的范围，如果数据值的范围过大，则建立统计数组的成本将会很高，而如果数据数量相对于数据值的范围过小，则会造成巨大的浪费（因为很对统计都是0）。

另外，需要注意的是，伪代码中的最后一个循环用的是downto而不是to，其实如果是对于纯数字的数据来说，用to和downto的实际效果是一样的，因为这些都是相同的数字。但是对于不以key值作为区分的数据来数，downto可以保证数据原本的相对位置，比如，a和b的key值相同，而且a原本就排在b前，则用downto的话还可以保证a先于b。这个的原理是，C\[k\]在最后输出时候的值其实是key值为k的元素所可以在的最高位置的下标，所以为了保证key值为k的元素还保留原来的相对位置，则应该将原本位置靠后的数据元素先放置在相对较高的位置上，则这样就可以保持原本数据的相对位置不变了。

其实，计数排序保证的所谓的相对位置是一种**稳定性**，一个排序算法被称为`稳定的`，则具有相同值的元素在输出数组中的相对次序与它们在输入数组中的相对次序相同。计数排序的稳定性之所以被提出来是因为基数排序需要稳定的子过程来保证其正确性。

计数排序的过程中，第一个循环的时间代价是`O(k)`，第二个循环的时间代价是`O(n)`，最后一个循环的时间代价也是`O(n)`，所以，总的时间就是`O(n+k)`。基数排序通常使用在当k=O(n)的情况下，这个时候算法的复杂度为`O(n)`。

### 基数排序

基数排序是一种用在老式穿卡机上的算法，每个卡片有80列，每次只能读到每一列的一位数字，所以，为了尽可能的保证一次输入就可以产生结果，出现了这样的算法。

![radix sort](http://kang-da.tk/images/post/radix-sort.png)

算法的流程是：

	RADIX-SORT(A, d)
		for i : 1 to d do
			use a stable sort to sort array A on digit i

这里的关键在于，每次**对各位排序的时候都要求是稳定排序**，主要是原因是，在一位一位的排序时，虽然当前排序的位的权值比前面已经排过的大，但是当数值出现相同时，则还是要保留前几位的排序相对位置，所以需要每一位上的排序是稳定的。另外，为什么不从高位开始排序呢？主要是因为其实先排的位的权值是相对低的，因为后面的排序可能会破坏先排好的序列，所以权值大的位要放在后面排。当然也不是不可以从高位进行排序，不过高位开始的基数排序和从低位开始的稍有差别，对于从高位开始的基数排序，除了第一轮以外，其他都是对前一位相同的数据元素进行在排序，而不是对全部元素进行再排序，这样就可以保证了后面的排序不会破坏前面排序的信息了，有递归的思想在里面。所以，在基数排序中，从低位开始的方法被称作LSD(Least significant digital)，从高位开始的被称作MSD(Most significant digital)。

关于基数排序的时间复杂度问题，因为基数排序的时间复杂度是`O(n+k)`，所以基数排序的时间复杂度就是`O(d(n+k))`，当d是常数的时候，算法还是保持线性时间。

基数排序从时间复杂度上来看是比快速排序要好的，但是，基数排序和快速排序时间复杂度中的常数是不同的，另外，快速排序的分治递归特性，使其更适合现有的硬件缓存结构，同时快速排序是原地排序，不需要额外的空间去存储中间数据，而基数排序则需要。综上，在实际效果上，快速排序要优于基数排序。

### 桶排序

桶排序的基本思想是，把现有的数据在数值区间中划分出n个子区间，如果输入数据是分布均匀的话，则落在每个区间内的元素数量是相当的，然后再对各个子区间内的元素进行排序，最终按照子区间依次输出即可。

桶排序也是有某些前提条件的，一个比较重要的就是要求输入的数据是尽可能随机产生的，即要求它们的分布是均匀的，这样的话，最终落在每个子区间内的元素数才会相对均衡。另外一点就是桶排序在使用过程中的划分，其实说的直白点就是**利用一个hash函数**把原数据分发到一个个hash桶里，然后再将hash桶中的元素进行排序，当然，这个hash函数的选择可能和原数据的数据特点有关系，要具体问题具体分析。从某些角度来看，桶排序和基数排序中的MSD还是有些相通的地方的，只不过MSD要递归下去，而桶排序不用递归。

![bucket sort](http://kang-da.tk/images/post/bucket-sort.png)

算法流程：

	BUCKET-SORT(A)
		N = length(A)
		for i : 1 to n do
			insert A[i] into list B[floor(nA[i])]
		for i : 0 to n-1 do
			sort list B[i] with insertion sort
		concatenate the lists B[0], B[1],...,B[n-1] together in order

算法流程中的A数组中的元素都是在区间\[0,1)中，所以第一个循环里的floor(nA\[i\])就相当于一个hash函数，只不过这里的hash函数比较简单，是以十分位的数值为基础hash的。

关于时间复杂度，从算法流程可以看出，两个循环都是O(n)的，但是第二个循环里有一个插入排序，而插入排序的时间复杂度是`O(n^2)`的，不过这里要进行插入排序的"n"进行期望求值之后是`2-1/n`，具体可以参考算法导论8.4节。所以桶排序的期望运行时间为`O(n)+n*O(2-1/n)=O(n)`。

另外，还有一点要注意，只要满足划分子区间的尺寸的平方和总元素数呈线性关系，那么即使输入元素不符合均匀分布，桶排序也可以以线性时间运行。具体的原因可以参考算法导论中的期望求值部分。

## 关于稳定性的讨论

稳定性应该算是排序算法的天然属性，当然有的算法可以通过修改来满足稳定性，但是多数的算法还是确定的。下面依次对上述算法的稳定性进行讨论。

### 插入排序（冒泡、选择）

插入排序我认为，在判断上的区别决定了它是否是稳定的，从低位开始比较，如果在插入的时候每次对前一个位置的元素做大于等于对后一个元素做小于操作，则这样的情况下插入排序是稳定的，因为这样的情况下，同样值的情况下先插入的会排在前面，这样保证排序是稳定的。在等于判断放在和后一个元素的判断上时这个顺序是被逆向的，不是稳定的。如果从高位开始也有类似的情况。

冒泡排序，如果两个相邻的元素相等，则不会做无用的交换，所以依然保留原有的相对顺序，是**稳定的**。

选择排序，每次都是选择无序区的极值元素，所以**在交换的时候可能造成当前位置元素被交换到与它值相同的元素位置之后**，例如(5,7,5,2,9)，按照从小到到的顺序排序，则第一遍的时候会将一个位置上的5与第四位置上的2交换，从而使两个5的相对顺序被破坏了。是**不稳定的**。但是，**如果是链表实现或者是新开一个数组存储排序后的数据的话，则是稳定的**，因为这样的情况下，不会产生上述的交换，只是每次遍历一遍序列然后挑出极值存起来而已。

### 归并排序

归并排序中涉及到交换的有两个地方，一个是递归出口，一个是合并过程。首先，在递归出口出，如果剩下一个元素，则不需要交换，如果剩下两个而且相等，没必要进行无为的交换。在合并过程中，**遵循队列头相同，前一个队列有限的原则则可以保证稳定性**。所以归并排序是**稳定的**。

### 堆排序

当进行堆调整的时候，例如，n/2的节点开始调整，将其儿子节点n交换到了n/2的位置，而n/2-1位置的节点有一个与原来n位置元素值相同的儿子n-2，但是没有交换，则造成原来的n-2和n位置上两个元素的相对位置发生了变化。**不稳定**。

### 快速排序

当进行划分的时候，主元被选为第一位置的元素，如果有与主元值相同的元素被放在较前的位置，在最后将主元交换到序列中间时会造成主元和这些有相同值的元素的相对位置被破坏，不论主元是如何选择的，这种最后的交换都很有可能破坏稳定性。**不稳定**。

### 计数排序

这个在前面说过了，也是要看最后循环的输出顺序了，不过，是可以保证**稳定的**。

### 基数排序

因为循环中的每一次子排序都是稳定的，所以最终整个排序也是**稳定的**。

### 桶排序

元素值相同的元素会被分派到同一个“桶”中，而每一个桶里的排序如果是稳定的，则整个桶排序自然是稳定的。

## 其他排序算法

其实还有一些其他的排序算法，例如我最爱的[二叉排序树](http://zh.wikipedia.org/wiki/%E4%BA%8C%E5%8F%89%E6%8E%92%E5%BA%8F%E6%A0%91)。

